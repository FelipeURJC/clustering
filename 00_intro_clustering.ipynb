{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "This notebook, by [felipe.alonso@urjc.es](mailto:felipe.alonso@urjc.es)\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [Kmeans](#kmeans)\n",
    "2. [Hierarchical clustering](#hierarchical)\n",
    "3. [DBSCAN](#dbscan)\n",
    "    1. [HDBSCAN](#hdbscan)\n",
    "4. [Mixture of Gaussians](#mixture)\n",
    "5. [Spectral clustering](#spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 0. Preliminaries \n",
    "\n",
    "Load libraries, data sets and useful functions. A good practice is to create auxiliary functions and/or classes in additional files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas, numpy and matplotlib\n",
    "from src.utils import pd, np, plt\n",
    "\n",
    "# Useful functions\n",
    "from src.utils import load_examples, plot_scatter, plot_silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 4 synthetic data sets for illustration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data sets\n",
    "X1, y1, X2, y2, X3, y3, X4, y4 = load_examples()\n",
    "\n",
    "# Do the plotting\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.subplot(221)\n",
    "plot_scatter(X1,'3 equal sized and densed clusters')\n",
    "\n",
    "plt.subplot(222)\n",
    "plot_scatter(X2,'Unequal variance')\n",
    "\n",
    "plt.subplot(223)\n",
    "plot_scatter(X3,'Anisotropic distribution')\n",
    "\n",
    "plt.subplot(224)\n",
    "plot_scatter(X4,'Two moons')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='kmeans'></a>\n",
    "# 1. K-means\n",
    "\n",
    "It is always good practice to take a look at the [user guide](https://scikit-learn.org/stable/modules/clustering.html#k-means) and [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) provided by scikit-learn.\n",
    "\n",
    "Let's start running k-means for the $X_1$, $y_1$ data set (top-left figure). We choose $k=3$ since it looks a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# build the clustering model\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters = k)\n",
    "kmeans.fit(X1)\n",
    "\n",
    "# Centroids \n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# do the plotting\n",
    "plot_scatter(X1,'k = ' + str(k), cluster_labels, centroids)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "<b>EXERCISE:</b> Change the value of $k$ and run the code again\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also assign cluster labels to new points, using the predict method. Each new point is assigned to the closest cluster center when predicting, but the existing model is not changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.predict(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 How do you determine the optimal value of $k$?\n",
    "\n",
    "Try running the algorithm for increasing $k$ and calculate the **inertia** (sum of squared distances of samples to their closest cluster center):\n",
    "\n",
    "$$\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)$$\n",
    "\n",
    "As $k$ increases, clusters become smaller, and inertia decreases. Plot this distance against the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(1,10)\n",
    "\n",
    "inertia = []\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k).fit(X1)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(K,inertia,'.-')\n",
    "plt.xlabel('# of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it looks like 3 is a good candidate for the optimal value of $k$.\n",
    "\n",
    "#### Silhouette coefficient\n",
    "\n",
    "The [silhouette analysis](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py) can be also used to select the number of clusters.\n",
    "\n",
    "For each sample in the data set, we can calculate a **silhouette coefficient** that measures of how close each point in one cluster is to points in the neighboring clusters. This coefficiente ranges from $-1$ to $1$, so that:\n",
    "- Near +1 indicate that the sample is far away from the neighboring clusters. \n",
    "- A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and \n",
    "- Negative values indicate that those samples might have been assigned to the wrong cluster.\n",
    "\n",
    "Silhouette coefficients can be plotted, thus providing a way to asses the number of clusters visually.\n",
    "\n",
    "We can also calculate an **average silhouette coefficient** which gives a perspective into the density and separation of the formed clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k).fit(X1)\n",
    "plot_silhouette(X1,k,kmeans.labels_,kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "<b>EXERCISE:</b> For the $X_1$ data set, represent the silhouette score for different values of $k$.\n",
    "</div>\n",
    "\n",
    "**Hint**: `silhouette_avg = silhouette_score(X, cluster_labels)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering performance evaluation\n",
    "\n",
    "There are other metrics that we might want to calculate to analyze the quality of the clustering:\n",
    "\n",
    "- **Cluster cardinality**: is the number of examples per cluster.\n",
    "- **Cluster magnitude**: is the sum of distances from all examples to the centroid of the cluster\n",
    "\n",
    "And many [others](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 More examples of k-means\n",
    "\n",
    "<div class = \"alert alert-info\">\n",
    "<b>EXERCISE:</b> Run the K-means for $X_2$, $X_3$, $X_4$ data sets. Use inertia to determine the number of clusters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 K-means some conclusions\n",
    "\n",
    "Notice that \n",
    "\n",
    "1. In k-means, each cluster is defined solely by its center, which means that each cluster is a convex shape. As a result of this, it can only capture relatively simple shapes. \n",
    "\n",
    "2. k-means also assumes that all clusters have the same “diameter” in some sense; it always draws the boundary between clusters to be exactly in the middle between the cluster centers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='hierarchical'></a>\n",
    "# 2. Hierarchical clustering\n",
    "\n",
    "Let's start with an interpretable data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(random_state=0, n_samples=12)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = plt.gca()\n",
    "for i, x in enumerate(X):\n",
    "    ax.text(x[0] + .15, x[1] + .15, \"%d\" % i, horizontalalignment='left', verticalalignment='center')\n",
    "\n",
    "ax.scatter(X[:,0],X[:,1], c = 'grey', s = 150, edgecolors = 'k')\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Scikit-learn` currently **does not have the functionality to draw dendrograms**. However, you can generate them easily using SciPy. The [SciPy clustering](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html#module-scipy.cluster.hierarchy) algorithms have a slightly different interface to the scikit-learn clustering algorithms. SciPy provides a function that takes a data array X and computes a linkage array, which encodes hierarchical cluster similarities. We can then feed this linkage array into the scipy dendrogram function to plot the dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "Z = linkage(X, 'ward')\n",
    "dendrogram(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned during the lecture:\n",
    "\n",
    "- The Y-axis contains the measure of similarity between observations.\n",
    "- The higher up the Y-axis, the more different two observations are.\n",
    "- The height at which we cut the dendogram gives us the number of clusters.\n",
    "- The same dendogram can be used to try out different numbers of clusters.\n",
    "\n",
    "\n",
    "<div class = \"alert alert-info\">\n",
    "<b>EXERCISE:</b> Take a look to the different linkage methods provided by SciPy.\n",
    "</div>\n",
    "\n",
    "Hint: [`scipy.cluster.hierarchy.linkag`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage)\n",
    "\n",
    "\n",
    "Once you decide the number of clusters you want to use using the dendrogram, you can use the scikit-learn function [AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) to cluster your data (you could do it also with SciPy [fcluster](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html#scipy.cluster.hierarchy.fcluster))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=3).fit(X)\n",
    "plot_scatter(X,'Hierarchical clustering', agg.labels_) # we do not have centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "<b>EXERCISE:</b> Use the dendrogram to determine the number of clusters for cases from $X_1$ to $X_4$. Then, plot the resulting clusters. Try using different linkage metrics if results are not as expected.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='dbscan'></a>\n",
    "# 3. DBSCAN\n",
    "\n",
    "Let’s apply DBSCAN on the synthetic dataset we used to demonstrate agglomerative clustering. Like agglomerative clustering, DBSCAN does not allow predictions on new test data, so we will use the fit_predict method to perform clustering and return the cluster labels in one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "dbscan_labels = dbscan.fit_predict(X)\n",
    "plot_scatter(X,'DBSCAN', dbscan_labels) # we do not have centroids\n",
    "plt.show()\n",
    "print(\"Cluster memberships:\\n{}\".format(dbscan_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all data points were assigned the label -1, which stands for noise. This is a consequence of the default parameter settings for `eps` and `min_samples`, which are not tuned for small toy datasets.\n",
    "\n",
    "<div class = \"alert alert-info\">\n",
    "<b>EXERCISE:</b> Try different values of $min\\_samples$ and $eps$ and print the cluster membership.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "\n",
    "- The parameter `eps` is somewhat more important, as it determines what it means for points to be “close.” Setting `eps` to be very small will mean that no points are core samples, and may lead to all points being labeled as noise. Setting `eps` to be very large will result in all points forming a single cluster.\n",
    "\n",
    "- The `min_samples` setting mostly determines whether points in less dense regions will be labeled as outliers or as their own clusters. If you increase `min_samples`, anything that would have been a cluster with less than `min_samples` many samples will now be labeled as noise. `min_samples` therefore determines the minimum cluster size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "<b>EXERCISE:</b> Use DBSCAN for clustering our cases data sets: from $X_1$ to $X_4$. Try different values of $eps$ and $min\\_samples$ to get the desirable cluster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, note that\n",
    "\n",
    "- When using DBSCAN, you need to be careful about handling the returned cluster assignments. **The use of -1 to indicate noise might result in unexpected effects** when using the cluster labels to index another array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hdbscan'></a>\n",
    "## 3.1 HDBSCAN\n",
    "\n",
    "First, take a look to the documentation which is available on [ReadTheDocs](http://hdbscan.readthedocs.io/en/latest/), specially to the [Parameter Selection](https://hdbscan.readthedocs.io/en/latest/parameter_selection.html#selecting-min-cluster-size) section which is summarized here:\n",
    "\n",
    "- `min_cluster_size`: which is how big a cluster needs to be in order to form (similar to `eps` in DBSCAN). \n",
    "- `min_samples`: provide a measure of how conservative you want you clustering to be. The larger the value of min_samples you provide, the more conservative the clustering – more points will be declared as noise, and clusters will be restricted to progressively more dense areas.\n",
    "\n",
    "Let's start with the same example as DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=3, min_samples = 1)\n",
    "hdbscan_labels = clusterer.fit_predict(X)\n",
    "\n",
    "plot_scatter(X,'HDBSCAN', hdbscan_labels) # we do not have centroids\n",
    "plt.show()\n",
    "print(\"Cluster memberships:\\n{}\".format(hdbscan_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen, `min_cluster_size` and `min_samples` do not provide obvious relationship, making it difficult to set up these values, specially in real world scenarios.\n",
    "\n",
    "<div class = \"alert alert-info\">\n",
    "<b>EXERCISE:</b> Use HDBSCAN for clustering our cases data sets: from $X_1$ to $X_4$. Try different values of $min\\_cluster\\_size$ to get the desirable cluster. If results are not as expected, then vary the value of $min\\_samples$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='mixture'></a>\n",
    "# 4. Mixture of gaussians\n",
    "\n",
    "As we mentioned, [mixture of gaussians](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture) model attempts to find a mixture of multi-dimensional Gaussian probability distributions that best model any input data set for finding clusters.\n",
    "\n",
    "Let's start with our $X_1$ example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "k = 3\n",
    "gmm = GaussianMixture(n_components = k).fit(X1)\n",
    "gmm_labels = gmm.predict(X1)\n",
    "\n",
    "# do the plotting\n",
    "plot_scatter(X1,'k = ' + str(k), gmm_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But because `gmm` contains a probabilistic model under the hood, it is also possible to find probabilistic cluster assignments. in Scikit-Learn this is done using the `predict_proba` method. This returns a matrix of size `[n_samples, n_clusters]` which measures the probability that any point belongs to the given cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm.predict_proba(X1).round(3)\n",
    "# What if we change the number of clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the gaussian distributions fitted to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import plot_gmm\n",
    "\n",
    "k = 3\n",
    "data = X1\n",
    "gmm = GaussianMixture(n_components = k)\n",
    "plot_gmm(gmm, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "<b>EXERCISE:</b> Use the above function to represent the resulting clusters for $X_2$, $X_3$ and $X_4$. Try different values of $k$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance type\n",
    "\n",
    "There's an extra parameter that affects the clustering process: `covariance_type`. With this parameter we control how covariance matrix is estimated and this affects in the **shape of each cluster**.\n",
    "\n",
    "Let's see how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "data = X3\n",
    "gmm = GaussianMixture(n_components = k, covariance_type = 'full')\n",
    "plot_gmm(gmm, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `covariance_type = 'full` allows each cluster to be modeled as an ellipse with arbitrary orientation. However, it is the most complicated and computationally expensive model (especially as the number of dimensions grows).\n",
    "- `covariance_type = 'spherical` constrains the shape of the cluster such that all dimensions are equal. The resulting clustering will have similar characteristics to that of k-means, though it is not entirely equivalent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of `n_components`\n",
    "\n",
    "The fact that `gmm` is a generative model gives us a natural means of determining the optimal number of components for a given dat set. A generative model is inherently a probability distribution for the dataset, and so we can simply evaluate the likelihood of the data under the model, using cross-validation to avoid over-fitting. \n",
    "\n",
    "Another means of correcting for over-fitting is to adjust the model likelihoods using some analytic criterion such as the [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) or the [Bayesian information criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion). Scikit-Learn's GMM estimator actually includes built-in methods that compute both of these, and so it is very easy to operate on this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = np.arange(1, 21)\n",
    "\n",
    "data = X4\n",
    "models = [GaussianMixture(n,random_state=0).fit(data) for n in n_components]\n",
    "\n",
    "plt.plot(n_components, [m.bic(data) for m in models], label='BIC')\n",
    "plt.plot(n_components, [m.aic(data) for m in models], label='AIC')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('n_components');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "data = X4\n",
    "gmm = GaussianMixture(n_components = k, covariance_type = 'full')\n",
    "plot_gmm(gmm, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='spectral'></a>\n",
    "# 5. Spectral Clustering\n",
    "\n",
    "Work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
